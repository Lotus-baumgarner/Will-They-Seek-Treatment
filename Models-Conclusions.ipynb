{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Will They Seek Treatment for a Mental Illness?\n",
    "\n",
    "__By Lotus Baumgarner__\n",
    "\n",
    "This is part two of two notebooks. This notebook covers the Models, Hyperparameter Tuning, Final Model Selection and Conclusion/Next Steps. The part one notebook covers the Data/Problem understanding, Data Cleaning, Visualizations and Initial Feature Selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization and Statistics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Preprocessing and Models\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Other Imports\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import os  \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\lotus\\\\Documents\\\\Flatiron\\\\Projects\\\\Phase5-CapstoneProject\\\\Data\\\\Treatment2_data.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split & Basic Pipeline Set-up:\n",
    "I used __Train-Test Split__ with a holdout set and __Cross-Validation__. I used a 80/20 split for my training_validation set and holdout set. Then I used a 75/25 split to split the train_validation set into seperate training and validation sets. I also mapped my target variable (y) to be Yes = 1 and No = 0.\n",
    "\n",
    "__I am labeling my TP, TN, FP, and FN as follows:__\n",
    "\n",
    "•\t__True Positives (TP):__ The number of individuals who were correctly predicted to seek treatment for a mental illness (i.e., the model predicted \"Yes\" for treatment, and the actual value was also \"Yes\").\n",
    "\n",
    "•\t__True Negatives (TN):__ The number of individuals who were correctly predicted not to seek treatment for a mental illness (i.e., the model predicted \"No\" for treatment, and the actual value was also \"No\").\n",
    "\n",
    "•\t__False Positives (FP):__ The number of individuals who were incorrectly predicted to seek treatment for a mental illness (i.e., the model predicted \"Yes\" for treatment, but the actual value was \"No\").\n",
    "\n",
    "•\t__False Negatives (FN):__ The number of individuals who were incorrectly predicted not to seek treatment for a mental illness (i.e., the model predicted \"No\" for treatment, but the actual value was \"Yes\").\n",
    "\n",
    "I will be focusing on __Precision__ as my metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected only the features with significant association. (See EDAs-Feature Selection Notebook)\n",
    "significant_features = ['Gender', 'Family_History', 'Mental_Health_Interview', 'Care_Options', \n",
    "                        'Self_Employed', 'Coping_Struggles', 'Growing_Stress']\n",
    "\n",
    "# Defined X and y to be split. And mapped y to numeric format.\n",
    "X = df[significant_features]\n",
    "y = df['Treatment'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# First split the dataset into train_validation and holdout sets (80/20)\n",
    "X_train_val, X_holdout, y_train_val, y_holdout = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then split the train_validation set into seperate training and validation sets (75/25)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printed shape of all training, validation, and holdout sets.\n",
    "print(\"The X Train_Validation set shape is \", X_train_val.shape)\n",
    "print(\"The X Train set shape is            \", X_train.shape)\n",
    "print(\"The X Validation set shape is       \", X_val.shape)\n",
    "print(\"The X Holdout set shape is          \", X_holdout.shape)\n",
    "print()\n",
    "print(\"The y Train_Validation set shape is \", y_train_val.shape)\n",
    "print(\"The y Train set shape is            \", y_train.shape)\n",
    "print(\"The y Validation set shape is       \", y_val.shape)\n",
    "print(\"The y Holdout set shape is          \", y_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created a preprocessing pipeline with OneHotEncoder for categorical features\n",
    "categorical_features = significant_features\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Defined a basic pipeline with an empty Model place\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', 'Model')\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4A. Model 1: Baseline - Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the Baseline model using Logistic Regression\n",
    "LogReg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Added LogReg to empty model slot on pipeline\n",
    "logreg_pipeline = pipeline.set_params(model=LogReg)\n",
    "\n",
    "logreg_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained the model\n",
    "logreg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions & Evaluation\n",
    "y_pred1 = logreg_pipeline.predict(X_val)\n",
    "accuracy1 = accuracy_score(y_val, y_pred1)\n",
    "classification_report1 = classification_report(y_val, y_pred1)\n",
    "\n",
    "# Cross-validated\n",
    "cv_scores1 = cross_val_score(logreg_pipeline, X_train_val, y_train_val, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"\\033[1mThe Accuracy score on the validation set is:\\033[0m \", accuracy1)\n",
    "print(\"\\033[1mClassification Report:\\033[0m\\n\", classification_report1)\n",
    "print(\"\\033[1mCross-validation precision scores:\\033[0m\\n\", cv_scores1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS:  Validation Set\n",
    "Keeping in mind that Yes = 1 and No = 0, The Precision score of 0.6866 means the model is predicting 69% of individuals who are likely to seek treatment on their own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the confusion matrix for visualization\n",
    "cm1 = confusion_matrix(y_val, y_pred1, labels=logreg_pipeline.named_steps['model'].classes_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm1, annot=True, fmt='d', cmap='Purples')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS: Validation Set\n",
    "__TP:__ The model correctly predicted __7,668__ individuals who would seek treatment for a mental illness.\n",
    "\n",
    "__TN:__ The model correctly predicted __6,485__ individuals who would not seek treatment for a mental illness.\n",
    "\n",
    "__FP:__ The model incorrectly predicted __3,499__ individuals as seeking treatment on their own when they will not.\n",
    "\n",
    "__FN:__ The model incorrectly predicted __2,348__ individuals as not seeking treatment on their own when they will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used Get Feature Name Out to get a list and total number of the columns created by OneHotEncoder\n",
    "ohe_feature_names = logreg_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out()\n",
    "\n",
    "total_new_columns = len(ohe_feature_names)\n",
    "\n",
    "print(\"\\033[1mThe list of new columns from OHE is:\\033[0m\\n\", ohe_feature_names)\n",
    "print()\n",
    "print(\"\\033[1mThe total number of new columns is:\\033[0m \", total_new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tested on the Holdout set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions & Evaluation on the holdout set\n",
    "y_holdout_pred1 = logreg_pipeline.predict(X_holdout)\n",
    "\n",
    "accuracy_holdout1 = accuracy_score(y_holdout, y_holdout_pred1)\n",
    "classification_report_holdout1 = classification_report(y_holdout, y_holdout_pred1)\n",
    "\n",
    "# Cross-validated\n",
    "cv_scores_holdout1 = cross_val_score(logreg_pipeline, X, y, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"\\033[1mThe Accuracy score on the holdout set is:\\033[0m \", accuracy_holdout1)\n",
    "print(\"\\033[1mClassification Report:\\033[0m\\n\", classification_report_holdout1)\n",
    "print(\"\\033[1mCross-validation precision scores:\\033[0m\\n\", cv_scores_holdout1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the confusion matrix for visualization\n",
    "cm_holdout1 = confusion_matrix(y_holdout, y_holdout_pred1, labels=logreg_pipeline.named_steps['model'].classes_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_holdout1, annot=True, fmt='d', cmap='Purples')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS Holdout Set:\n",
    "Overall, the model's performance is similar to what was observed on the validation set (0.6866 Validation and 0.6777 Holdout), indicating that the model is generalizing well to unseen data. The TP, TN, FP, FN are all close to eachother as well.\n",
    "\n",
    "__TP:__ Validation Set: __7,668__ -------- Holdout Set: __7,578__ \n",
    "\n",
    "__TN:__ Validation Set: __6,485__ -------- Holdout Set: __6,475__\n",
    "\n",
    "__FP:__ Validation Set: __3,499__ -------- Holdout Set: __3,603__\n",
    "\n",
    "__FN:__ Validation Set: __2,348__ -------- Holdout Set: __2,344__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B. Model 2:  Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined Random Forest Classifier as my 2nd model\n",
    "RandForest = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Added RandForest to empty model slot on pipeline\n",
    "RF_pipeline = pipeline.set_params(model=RandForest)\n",
    "\n",
    "RF_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained the model\n",
    "RF_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions & Evaluation\n",
    "y_pred2 = RF_pipeline.predict(X_val)\n",
    "accuracy2 = accuracy_score(y_val, y_pred2)\n",
    "classification_report2 = classification_report(y_val, y_pred2)\n",
    "\n",
    "# Cross-validated\n",
    "cv_scores2 = cross_val_score(RF_pipeline, X_train_val, y_train_val, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"\\033[1mThe Accuracy score on the validation set is:\\033[0m \", accuracy2)\n",
    "print(\"\\033[1mClassification Report:\\033[0m\\n\", classification_report2)\n",
    "print(\"\\033[1mCross-validation precision scores:\\033[0m\\n\", cv_scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS:  Validation Set\n",
    "Keeping in mind that Yes = 1 and No = 0, The Precision score of 0.6893 means the model is still predicting 69% of individuals who are likely to seek treatment on their own correctly.  But it increased to predicting 78% of individuals who are unlikely to seek treatment on their own correctly now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the confusion matrix for visualization\n",
    "cm2 = confusion_matrix(y_val, y_pred2, labels=RF_pipeline.named_steps['model'].classes_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm2, annot=True, fmt='d', cmap='Purples')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS: Validation Set\n",
    "__TP:__ The model correctly predicted __8,286__ individuals who would seek treatment for a mental illness.\n",
    "\n",
    "__TN:__ The model correctly predicted __6,250__ individuals who would not seek treatment for a mental illness.\n",
    "\n",
    "__FP:__ The model incorrectly predicted __3,734__ individuals as seeking treatment on their own when they will not.\n",
    "\n",
    "__FN:__ The model incorrectly predicted __1,730__ individuals as not seeking treatment on their own when they will.\n",
    "\n",
    "While Random Forest did increase the number of True Positives (7,668 --> 8,286), it also increased the number of False Positives (3,499 --> 3,734).  \n",
    "And while it did decrease the number of False Negatives (2,348 --> 1,730), it also decreased the number of True Negatives (6,485 --> 6,250).  \n",
    "I'm hoping that by using __Hyperparameter Tuning__ I'll be able to decrease my False Positives and increase my overall Precision Score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV:\n",
    "Used GridSearchCV to find the best hyperparameters and model.  Then applied that model to the pipeline and retested against the validation and holdout sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid for hyperparameter tuning\n",
    "RF_param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search_RF = GridSearchCV(RF_pipeline, RF_param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV to the train_val set to find the best parameters\n",
    "grid_search_RF.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Best hyperparameters and the corresponding best model\n",
    "best_RF_params = grid_search_RF.best_params_\n",
    "best_RF_model = grid_search_RF.best_estimator_\n",
    "\n",
    "\n",
    "print(\"\\033[1mThe Best Parameters are:\\033[0m\\n\", best_RF_params)\n",
    "print()\n",
    "print(\"\\033[1mThe Best Model is:\\033[0m\\n\", best_RF_model)\n",
    "print()\n",
    "print(\"\\033[1mThe Best Accuracy Score is: \\033[0m\", best_RF_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy on the validation set\n",
    "val_accuracy = best_RF_model.score(X_val, y_val)\n",
    "\n",
    "# Evaluate accuracy on the holdout set\n",
    "holdout_accuracy = best_RF_model.score(X_holdout, y_holdout)\n",
    "\n",
    "print(\"The Validation Set Accuracy Score is: \", val_accuracy)\n",
    "print(\"Holdout Set Accuracy:\", holdout_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the confusion matrix for visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4C. Model 3:  XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined XGBClassifier as my 3rd model\n",
    "XGBClass = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Added XGBClass to empty model slot on pipeline\n",
    "XGBClass_pipeline = pipeline.set_params(model=XGBClass)\n",
    "\n",
    "XGBClass_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained the model\n",
    "XGBClass_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions & Evaluation\n",
    "y_pred3 = XGBClass_pipeline.predict(X_val)\n",
    "precision3 = precision_score(y_val, y_pred3)\n",
    "classification_report3 = classification_report(y_val, y_pred3)\n",
    "\n",
    "# Cross-validated\n",
    "cv_scores3 = cross_val_score(XGBClass_pipeline, X_train_val, y_train_val, cv=5, scoring='precision')\n",
    "\n",
    "print(\"The Precision score on the validation set is: \", precision3)\n",
    "print(\"Classification Report:\\n\", classification_report3)\n",
    "print(\"Cross-validation precision scores:\\n\", cv_scores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS: Validation Set\n",
    "Keeping in mind that Yes = 1 and No = 0, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created the confusion matrix for visualization\n",
    "cm3 = confusion_matrix(y_val, y_pred3, labels=XGBClass_pipeline.named_steps['model'].classes_)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm3, annot=True, fmt='d', cmap='Purples')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "\n",
    "plt.xticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "plt.yticks(ticks=[0.5, 1.5], labels=['No', 'Yes'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FINDINGS: Validation Set\n",
    "__TP:__ The model correctly predicted __8,286__ individuals who would seek treatment for a mental illness.\n",
    "\n",
    "__TN:__ The model correctly predicted __6,250__ individuals who would not seek treatment for a mental illness.\n",
    "\n",
    "__FP:__ The model incorrectly predicted __3,734__ individuals as seeking treatment on their own when they will not.\n",
    "\n",
    "__FN:__ The model incorrectly predicted __1,730__ individuals as not seeking treatment on their own when they will.\n",
    "\n",
    "While Random Forest did increase the number of True Positives (7,668 --> 8,286), it also increased the number of False Positives (3,499 --> 3,734).  \n",
    "And while it did decrease the number of False Negatives (2,348 --> 1,730), it also decreased the number of True Negatives (6,485 --> 6,250).  \n",
    "I'm hoping that by using __Hyperparameter Tuning__ I'll be able to decrease my False Positives and increase my overall Precision Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridSearchCV:\n",
    "Used GridSearchCV again to find the best hyperparameters and model.  Then applied that model to the pipeline and retested against the validation and holdout sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined XGB Classifier with the new parameters\n",
    "param_grid_XGBClass = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [3, 4, 5],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search_XGBClass = GridSearchCV(XGBClass_pipeline, param_grid_XGBClass, cv=5, scoring='precision')\n",
    "\n",
    "# Fit GridSearchCV to the train_val set to find the best parameters\n",
    "grid_search_XGBClass.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Best hyperparameters and the corresponding best model\n",
    "best_XGBClass_params = grid_search_XGBClass.best_params_\n",
    "best_XGBClass_model = grid_search_XGBClass.best_estimator_\n",
    "best_XGBClass_score = grid_search_XGBClass.best_score_\n",
    "\n",
    "print(\"\\033[1mThe Best Parameters are:\\033[0m\\n\", best_XGBClass_params)\n",
    "print()\n",
    "print(\"\\033[1mThe Best Model is:\\033[0m\\n\", best_XGBClass_model)\n",
    "print()\n",
    "print(\"\\033[1mThe Best Precision Score is: \\033[0m\", best_XGBClass_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
